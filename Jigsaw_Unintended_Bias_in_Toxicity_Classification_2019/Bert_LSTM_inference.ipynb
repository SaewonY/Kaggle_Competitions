{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nfrom pathlib import Path\nimport os\nimport time\nimport gc\nimport random\nimport pickle\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\n# import torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\n\nfrom gensim.models import KeyedVectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    # reference: https://www.kaggle.com/coolcoder22/lightgbm-fast-compact-solution\n    return len(re.findall(regexp, text))\n\ndef normal_feature_engineering(text_list):\n    add_feats_matrix = np.zeros((len(text_list), 11))\n    \n    for idx in range(len(text_list)):\n        target_text = text_list[idx]\n        # num words\n        add_feats_matrix[idx, 0] = len(target_text.split(' '))\n        # unique num words\n        add_feats_matrix[idx, 1] = len(set(target_text.split(' ')))\n        # url\n        add_feats_matrix[idx, 2] = count_regexp_occ(r'http[s]{0,1}://\\S+', target_text)\n        # mail \n        add_feats_matrix[idx, 3] = count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', target_text)\n        # all upper words\n        add_feats_matrix[idx, 4] = count_regexp_occ(r'^[A-Z]+[A-Z]$', target_text)\n        \n        # symbols 1\n        symbol_cnt = 0\n        for symbol in symbols_to_isolate:\n            if symbol in target_text:\n                symbol_cnt += 1\n        \n        add_feats_matrix[idx, 5] = symbol_cnt\n        \n        # symbols 2\n        symbol_delete_cnt = 0\n        for symbol_del in symbols_to_delete:\n            if symbol_del in target_text:\n                symbol_delete_cnt += 1\n        \n        add_feats_matrix[idx, 6] = symbol_delete_cnt\n\n        # rate features\n        add_feats_matrix[idx, 7] = add_feats_matrix[idx, 1] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 8] = add_feats_matrix[idx, 4] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 9] = add_feats_matrix[idx, 5] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 10] = add_feats_matrix[idx, 6] / add_feats_matrix[idx, 0]\n    \n    print('normal feature enginnering done: {}'.format(add_feats_matrix.shape))\n    return add_feats_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        try:\n                            embedding_matrix[i] = embedding_index[word.upper()]\n                        except KeyError:\n                            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, normal_feats_idx, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        self.normal_feats_idx = normal_feats_idx\n\n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        normal_feats = batch[self.normal_feats_idx]\n\n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        batch[self.normal_feats_idx] = normal_feats\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n    \ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.normal_linear = nn.Linear(ADD_FEATS_UNITS, DENSE_HIDDEN_UNITS) \n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets) # num_aux_targets = 6 \n        \n    def forward(self, x, normal_feats, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        normal_linear  = F.relu(self.normal_linear(normal_feats.float()))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2 + normal_linear\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1) # out = 7\n        \n        return out\n    \n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nMAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nbatch_size = 512\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n# bert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\nbert_config = BertConfig('../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'+'bert_config.json')\n\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BERT Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_bert_list = os.listdir('../input/vanilla-epoch1/')\nbert_full_list = os.listdir('../input/bert-full-epoch1/')\n\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MY_BERT_PATH = Path('../input/vanilla-epoch1/')\nBERT_FULL_PATH = Path('../input/bert-full-epoch1/')\n\nbert_test_preds = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfor i, weight in enumerate(my_bert_list):\n    if i == 4:\n        break\n    print(\"{} bert model prediction starts\".format(i+1))\n    model = BertForSequenceClassification(bert_config, num_labels=7)\n    model.load_state_dict(torch.load(MY_BERT_PATH / weight))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n\n    test_preds = np.zeros((len(X_test)))\n    for i, (x_batch,) in enumerate(test_loader):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    bert_test_preds.append(test_pred)\n    del test_pred, model\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, weight in enumerate(bert_full_list):\n    print(\"{} bert model prediction starts\".format(i+1))\n    model = BertForSequenceClassification(bert_config, num_labels=7)\n#     state = torch.load(OMER_BERT_PATH / weight)\n    model.load_state_dict(torch.load(BERT_FULL_PATH / weight))\n#     model.load_state_dict(state['model_state_dict'])\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n\n    test_preds = np.zeros((len(X_test)))\n    for i, (x_batch,) in enumerate(test_loader):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    bert_test_preds.append(test_pred)\n    del test_pred, model\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_test_preds = np.mean(bert_test_preds, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_bert = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': bert_test_preds\n})\nsubmission_bert.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_bert.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_dataset\ndel test_loader\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\nPARA_EMBEDDING_PATH = '../input/pickled-paragram-300-vectors-sl999/paragram_300_sl999.pkl'\n\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nmaxlen = 220\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'))\n\nwith open('../input/toxic2019-preprocessed/x_train.pkl', 'rb') as f:    \n    x_train = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\nmax_features = 400000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_add_feats_matrix = normal_feature_engineering(test_df[\"comment_text\"].values)\n\nADD_FEATS_UNITS = test_add_feats_matrix.shape[1]\n\ntest_normal_feats = torch.from_numpy(test_add_feats_matrix)\n\ndel test_add_feats_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\n\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_test(model, test_loader):\n        \n    model.eval()\n    test_preds = np.zeros((len(x_test)))\n    test_preds_fold = np.zeros((len(x_test)))\n    with torch.no_grad():\n        for i, x in enumerate(test_loader):\n            x[0] = x[0].cuda()\n            x[1] = x[1].cuda()\n            x[2] = x[2].cuda()\n            \n            y_pred = model(x[0], x[1], x[2]).detach()\n            test_preds_fold[i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            \n        test_preds += test_preds_fold\n            \n    return test_preds\n\n\ndef fold_load_model(path):\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    temp_dict = torch.load(path)\n    temp_dict['embedding.weight'] = torch.tensor(embedding_matrix)\n    model.load_state_dict(temp_dict)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix, unknown_words_crawl\ndel glove_matrix, unknown_words_glove\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_1 = tokenizer.texts_to_sequences(x_test)\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test_1, maxlen=maxlen))\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test_1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\ntest_dataset = data.TensorDataset(x_test_padded, test_normal_feats, test_lengths)\ntest_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=2, normal_feats_idx=1)\n\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_lengths\ndel x_test_padded\ndel test_dataset\ndel test_collator\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# glove_crawl prediction\nglove_crawl_seed1 = { \n              'fold1': Path('../input/toxic-glove-crawl/10_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/10_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/10_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/10_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/10_5fold.pt5'),\n             }\nglove_crawl_seed2 = { \n              'fold1': Path('../input/toxic-glove-crawl/20_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/20_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/20_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/20_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/20_5fold.pt5'),\n             }\nglove_crawl_seed3 = { \n              'fold1': Path('../input/toxic-glove-crawl/30_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/30_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/30_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/30_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/30_5fold.pt5'),\n             }\nglove_crawl_seed4 = { \n              'fold1': Path('../input/toxic-glove-crawl/40_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/40_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/40_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/40_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/40_5fold.pt5'),\n             }\nglove_crawl_seed5 = { \n              'fold1': Path('../input/toxic-glove-crawl/50_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/50_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/50_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/50_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/50_5fold.pt5'),\n             }\nglove_crawl_seed6 = { \n              'fold1': Path('../input/toxic-glove-crawl/60_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/60_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/60_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/60_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/60_5fold.pt5'),\n            }            \nglove_crawl_seed7 = { \n              'fold1': Path('../input/toxic-glove-crawl/70_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/70_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/70_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/70_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/70_5fold.pt5'),\n            }            \nglove_crawl_seed8 = { \n              'fold1': Path('../input/toxic-glove-crawl/80_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/80_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/80_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/80_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/80_5fold.pt5'),\n            }            \nglove_crawl_seed9 = { \n              'fold1': Path('../input/toxic-glove-crawl/90_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/90_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/90_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/90_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/90_5fold.pt5'),\n            }            \nglove_crawl_seed10 = { \n              'fold1': Path('../input/toxic-glove-crawl/100_1fold.pt5'),\n              'fold2': Path('../input/toxic-glove-crawl/100_2fold.pt5'),\n              'fold3': Path('../input/toxic-glove-crawl/100_3fold.pt5'),\n              'fold4': Path('../input/toxic-glove-crawl/100_4fold.pt5'),\n              'fold5': Path('../input/toxic-glove-crawl/100_5fold.pt5'),\n            }            \n\n\nseed_list = [glove_crawl_seed1, glove_crawl_seed2, glove_crawl_seed3, glove_crawl_seed4, glove_crawl_seed5, \n             glove_crawl_seed6, glove_crawl_seed7, glove_crawl_seed8, glove_crawl_seed9, glove_crawl_seed10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nall_test_preds = []\n\nfor i, seed in enumerate(seed_list):\n    \n    print(\"glove_crawl seed {} prediction starts\".format(i+1))\n    \n    # for each loop adds 5 predictions\n    fold_preds = np.zeros((len(x_test)))\n\n    # 한 개 seed마다 fold prediction\n    for fold, path in seed.items():\n#         print(\"{}\".format(fold))\n        model = fold_load_model(path)\n        model.cuda()\n\n        test_preds = predict_test(model, test_loader)\n        fold_preds += test_preds\n        del test_preds, model\n        gc.collect()\n        \n    all_test_preds.append(fold_preds)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_predict = np.zeros((len(x_test)))\nfor _ in all_test_preds:\n    submission_predict += _","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_chris = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': submission_predict / 10\n})\n# submission_glove_crawl.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Omer_lstms"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(path, lstm_units=128):\n    model = omer_lstm(embedding_matrix, 6, lstm_units)\n    temp_dict = torch.load(path)\n    temp_dict['embedding.weight'] = torch.tensor(embedding_matrix)\n    model.load_state_dict(temp_dict)\n    model.cuda()\n    model.eval()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model):\n\n    all_test_preds = []\n    test_preds = np.zeros((len(x_test), 7)) # output_dim=7\n\n    for i, x_batch in enumerate(test_loader):\n    \n        X = x_batch[0].cuda()    \n        test_preds[i * batch_size:(i+1) * batch_size, :] = sigmoid(model(X).detach().cpu().numpy())\n\n    all_test_preds.append(test_preds)\n    del model\n    gc.collect()\n    return(all_test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix_1(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class omer_lstm(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets, lstm_units):\n        super(omer_lstm, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        self.DENSE_HIDEN_UNITS = lstm_units * 4\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, lstm_units, bidirectional=True, batch_first=True) \n        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(self.DENSE_HIDEN_UNITS, self.DENSE_HIDEN_UNITS)\n        self.linear2 = nn.Linear(self.DENSE_HIDEN_UNITS, self.DENSE_HIDEN_UNITS)\n        \n        self.linear_out = nn.Linear(self.DENSE_HIDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(self.DENSE_HIDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 300\nbatch_size = 128\n\nx_test = tokenizer.texts_to_sequences(x_test)\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ndel test_lengths\ndel x_test_padded\ndel test_dataset\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix_1(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix_1(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nprint(\"max features :\",max_features)\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# lstm 128 prediction\nomer_lstm_preds=[]\nlstm128_num = len(os.listdir('../input/omer-lstm-128'))\n\nfor _ in range(lstm128_num):\n    PATH = '../input/omer-lstm-128/modellstm'+str(_)+'.pt'\n    model = load_model(PATH)\n    all_test_preds = predict(model)\n    test_preds = all_test_preds[-1]\n    test_preds  = np.mean(all_test_preds, axis=0)[:, 0]\n    omer_lstm_preds.append(test_preds)\n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n# lstm 256 prediction\n\n# DENSE_HIDDEN_UNITS = 1024\n\nlstm256_num = len(os.listdir('../input/toxic-lstm256'))\n\nfor _ in range(lstm256_num):\n    PATH = '../input/toxic-lstm256/lstm256_'+str(_)+'.pt'\n    model = load_model(PATH, 256)\n    all_test_preds = predict(model)\n    test_preds = all_test_preds[-1]\n    test_preds  = np.mean(all_test_preds, axis=0)[:, 0]\n    omer_lstm_preds.append(test_preds)\n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"omer_lstm_preds = np.mean(omer_lstm_preds, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_omer = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': omer_lstm_preds\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensemble mine and Omer's lstms\nlstm_ensemble_prediction = submission_chris.prediction*0.5 + submission_omer.prediction*0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Blending part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nsubmission['prediction'] = ((lstm_ensemble_prediction + submission_bert.prediction) / 2 + submission_bert.prediction)/2 # => weighting more to berts\n# submission['prediction'] = ((lstm_ensemble_prediction + submission_bert.prediction) / 2 + lstm_ensemble_prediction)/2\n# submission['prediction'] = (lstm_ensemble_prediction + submission_bert.prediction) / 2\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}