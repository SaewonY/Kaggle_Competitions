{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\n\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR\nimport re\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JigsawEvaluator(object):\n    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n        self.y = (y_true >= 0.5).astype(int)\n        self.y_i = (y_identity >= 0.5).astype(int)\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n\n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n\n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        return overall_score + bias_score\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\nPARA_EMBEDDING_PATH = '../input/pickled-paragram-300-vectors-sl999/paragram_300_sl999.pkl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"LSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        try:\n                            embedding_matrix[i] = embedding_index[word.upper()]\n                        except KeyError:\n                            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.normal_linear = nn.Linear(ADD_FEATS_UNITS, DENSE_HIDDEN_UNITS)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets) # num_aux_targets = 6 \n        \n    def forward(self, x, normal_feats, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        normal_linear  = F.relu(self.normal_linear(normal_feats.float()))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2 + normal_linear\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1) # out = 7\n        \n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SequenceBucketCollator():\n    def __init__(self, choose_length, normal_feats_idx, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        self.normal_feats_idx = normal_feats_idx\n\n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        normal_feats = batch[self.normal_feats_idx]\n\n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        batch[self.normal_feats_idx] = normal_feats\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\nsymbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    # reference: https://www.kaggle.com/coolcoder22/lightgbm-fast-compact-solution\n    return len(re.findall(regexp, text))\n\ndef normal_feature_engineering(text_list):\n    add_feats_matrix = np.zeros((len(text_list), 11))\n    \n    for idx in range(len(text_list)):\n        target_text = text_list[idx]\n        # num words\n        add_feats_matrix[idx, 0] = len(target_text.split(' '))\n        # unique num words\n        add_feats_matrix[idx, 1] = len(set(target_text.split(' ')))\n        # url\n        add_feats_matrix[idx, 2] = count_regexp_occ(r'http[s]{0,1}://\\S+', target_text)\n        # mail \n        add_feats_matrix[idx, 3] = count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', target_text)\n        # all upper words\n        add_feats_matrix[idx, 4] = count_regexp_occ(r'^[A-Z]+[A-Z]$', target_text)\n        \n        # symbols 1\n        symbol_cnt = 0\n        for symbol in symbols_to_isolate:\n            if symbol in target_text:\n                symbol_cnt += 1\n        \n        add_feats_matrix[idx, 5] = symbol_cnt\n        \n        # symbols 2\n        symbol_delete_cnt = 0\n        for symbol_del in symbols_to_delete:\n            if symbol_del in target_text:\n                symbol_delete_cnt += 1\n        \n        add_feats_matrix[idx, 6] = symbol_delete_cnt\n\n        # rate features\n        add_feats_matrix[idx, 7] = add_feats_matrix[idx, 1] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 8] = add_feats_matrix[idx, 4] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 9] = add_feats_matrix[idx, 5] / add_feats_matrix[idx, 0]\n        add_feats_matrix[idx, 10] = add_feats_matrix[idx, 6] / add_feats_matrix[idx, 0]\n    \n    print('normal feature enginnering done: {}'.format(add_feats_matrix.shape))\n    return add_feats_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1]) # 2번째 weight를 기반, 1번째 column이 input / target에서 1번째 칼럼이 아웃풋\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:]) # output으로 내뱉는게 7개 칼럼\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open('../input/toxic2019-preprocessed/x_train.pkl', 'rb') as f:    \n    x_train = pickle.load(f)\n\nwith open('../input/toxic2019-preprocessed/x_test.pkl', 'rb') as f:\n    x_test = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n\n# for stratified kfold split\ntarget_binary = (train['target'].values > 0.5).astype(int)\n\n# saving for cv\ny_true = (train['target'].values > 0.5).astype(int)\ny_identity = train[identity_columns].values\n\nmax_features = 400000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_add_feats_matrix = normal_feature_engineering(train[\"comment_text\"].values)\n# test_add_feats_matrix = normal_feature_engineering(test[\"comment_text\"].values)\n\nADD_FEATS_UNITS = train_add_feats_matrix.shape[1]\nprint(\"ADD_FEATS_UNITS: {}\".format(ADD_FEATS_UNITS))\n\nnormal_feats = torch.from_numpy(train_add_feats_matrix)\n# test_normal_feats = torch.from_numpy(test_add_feats_matrix)\n\ndel train_add_feats_matrix  # , test_add_feats_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words = max_features, filters='', lower=False)\n\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# para_matrix, unknown_words_para = build_matrix(tokenizer.word_index, PARA_EMBEDDING_PATH)\n# print('n unknown words (para): ', len(unknown_words_para))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\n# embedding_matrix = para_matrix\n# embedding_matrix = crawl_matrix*0.7 + glove_matrix*0.3\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n# embedding_matrix = np.concatenate([para_matrix, crawl_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\n# del para_matrix, unknown_words_para\ndel glove_matrix, unknown_words_glove\ndel crawl_matrix, unknown_words_crawl\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix = np.mean(np.array([crawl_matrix, glove_matrix]), axis=0)\n# embedding_matrix.shape\n\n# embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n# embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_embed = embedding_matrix.shape[1]\n# embed_sampling = 400\n# idx = np.random.permutation(n_embed)[:embed_sampling]\n# embedding_matrix = embedding_matrix[:, idx]\n# print(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\n\nlengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\nmaxlen = 220\n# maxlen = lengths.max() \n\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\n\ndel x_train\ndel x_test\ndel tokenizer\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(train_loader, valid_loader, model, lr=0.001, \n                n_epochs=4, weight_file_name='fold.pt'):\n    \n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n    \n\n    best_valid_loss = 1.0\n    best_valid_fold = np.zeros((X_valid.size(0)))\n\n    for epoch in range(n_epochs):\n        start_time = time.time()\n                \n        model.train()\n        avg_train_loss = 0\n        \n        for i, (x, y) in enumerate(train_loader):\n            \n            x[0] = x[0].cuda()\n            x[1] = x[1].cuda()\n            x[2] = x[2].cuda()\n            y = y.cuda()\n            \n            y_pred = model(x[0], x[1], x[2])            \n            loss = custom_loss(y_pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            avg_train_loss += loss.item() / len(train_loader)\n            \n        avg_val_loss, valid_preds_fold = validation(model, valid_loader)\n        \n        elapsed = time.time() - start_time\n        \n        \n        temp_dict = model.state_dict()\n        del temp_dict['embedding.weight']\n        torch.save(temp_dict, weight_file_name+str(epoch))\n            \n        if avg_val_loss < best_valid_loss:\n            best_epoch = epoch + 1\n            best_valid_loss = avg_val_loss   \n            best_valid_fold = valid_preds_fold\n            \n        print(\"Epoch {} - avg_train_loss: {:.4f}  avg_val_loss: {:.4f}  lr: {:0.6f}  file: {}  time: {:.0f}s\".format(\n            epoch+1, avg_train_loss, avg_val_loss, scheduler.get_lr()[0], weight_file_name+str(epoch+1), elapsed))\n\n        scheduler.step()\n\n\n    return best_valid_fold, {\n        'best_epoch': best_epoch,\n        'best_loss': best_valid_loss,\n        'weight_file_name' : weight_file_name,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation(model, valid_loader):\n    \n    model.eval()\n    valid_preds_fold = np.zeros((X_valid.size(0)))\n    avg_val_loss = 0.\n    \n    with torch.no_grad():\n        for i, (x, y) in enumerate(valid_loader):\n            x[0] = x[0].cuda()\n            x[1] = x[1].cuda()\n            x[2] = x[2].cuda()\n            y = y.cuda()\n            \n            y_pred = model(x[0], x[1], x[2]).detach()\n            avg_val_loss += custom_loss(y_pred, y).item() / len(valid_loader)\n            valid_preds_fold[i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n    return avg_val_loss, valid_preds_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\n\nsplits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(x_train_padded, target_binary))\n\nfor seed in [90, 100]:\n\n    print(\"===== {} seed training starts ====== \".format(seed))\n    seed_everything(seed)\n\n    train_preds = np.zeros((len(x_train_padded)))\n    \n    for fold in [0,1,2,3,4]:\n\n        print(\"{0}/5 fold training starts!\".format(fold+1))\n\n        fold_num = str(fold + 1)\n\n        trn_index, val_index = splits[fold]\n\n        X_train, X_valid = x_train_padded[trn_index], x_train_padded[val_index]\n        train_normal_feats, val_normal_feats = normal_feats[trn_index], normal_feats[val_index]\n        train_length, valid_length = lengths[trn_index], lengths[val_index]\n        Y_train, Y_valid = y_train_torch[trn_index], y_train_torch[val_index]\n\n\n        train_dataset = data.TensorDataset(X_train, train_normal_feats, train_length, Y_train)\n        valid_dataset = data.TensorDataset(X_valid, val_normal_feats, valid_length, Y_valid)\n\n\n        train_collator = SequenceBucketCollator(lambda lengths: lengths.max(), sequence_index=0, length_index=2, normal_feats_idx=1, label_index=3)\n\n        train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n        valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n        \n        model = NeuralNet(embedding_matrix, y_aux_train.shape[-1]) # 6\n        model.cuda()\n        best_valid_fold, result = train_model(train_loader, valid_loader, model, lr=0.001, n_epochs=6, weight_file_name=str(seed)+'_' + fold_num + 'fold.pt')\n        print(result)\n\n        train_preds[val_index] = best_valid_fold\n        \n        del model, train_loader, valid_loader, best_valid_fold\n        gc.collect()\n\n    evaluator = JigsawEvaluator(y_true, y_identity)\n    auc_score = evaluator.get_final_metric(train_preds)\n    print(f'cv score: {auc_score:<8.5f}')\n    print()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}