{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features covered in this notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using vanila Unet Architecture\n",
    "- K-fold cross validation\n",
    "- loss function implemented\n",
    "- optimizing threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:36.356473Z",
     "start_time": "2020-01-22T10:53:32.804202Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext line_profiler\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm as tq\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:36.396680Z",
     "start_time": "2020-01-22T10:53:36.359685Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def resize_it(x):\n",
    "    if x.shape != (350, 525):\n",
    "        x = cv2.resize(x, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame = None,\n",
    "        datatype: str = \"train\",\n",
    "        img_ids: np.array = None,\n",
    "        transforms=albu.Compose([albu.HorizontalFlip()]), #, AT.ToTensor()\n",
    "        ):\n",
    "        self.df = df\n",
    "        if datatype != \"test\":\n",
    "            self.data_folder = f\"{path}/train_images/\"\n",
    "        else:\n",
    "            self.data_folder = f\"{path}/test_images/\"\n",
    "        self.img_ids = img_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.img_ids[idx]\n",
    "        mask = make_mask(self.df, image_name)\n",
    "        image_path = os.path.join(self.data_folder, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = np.transpose(augmented[\"image\"], [2, 0, 1])\n",
    "        mask = np.transpose(augmented[\"mask\"], [2, 0, 1])\n",
    "        return img, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:36.471706Z",
     "start_time": "2020-01-22T10:53:36.399826Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: str = \"\", shape: tuple = (1400, 2100)):\n",
    "    \"\"\"\n",
    "    Decode rle encoded mask.\n",
    "\n",
    "    :param mask_rle: run-length as string formatted (start length)\n",
    "    :param shape: (height, width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order=\"F\")\n",
    "\n",
    "\n",
    "def make_mask(df: pd.DataFrame, image_name: str = \"img.jpg\", shape: tuple = (350, 525)):\n",
    "    \"\"\"\n",
    "    Create mask based on df, image name and shape.\n",
    "    \"\"\"\n",
    "    masks = np.zeros((shape[0], shape[1], 4), dtype=np.float32)\n",
    "    df = df[df[\"im_id\"] == image_name]\n",
    "    for idx, im_name in enumerate(df[\"im_id\"].values):\n",
    "        for classidx, classid in enumerate([\"Fish\", \"Flower\", \"Gravel\", \"Sugar\"]):\n",
    "            mask = cv2.imread(\n",
    "                \"../input/understanding-clouds-resized/train_masks_525/train_masks_525/\"\n",
    "                + classid\n",
    "                + im_name\n",
    "            )\n",
    "            if mask is None:\n",
    "                continue\n",
    "            if mask[:, :, 0].shape != (350, 525):\n",
    "                mask = cv2.resize(mask, (525, 350))\n",
    "            masks[:, :, classidx] = mask[:, :, 0]\n",
    "    masks = masks / 255\n",
    "    return masks\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert image or mask.\n",
    "    \"\"\"\n",
    "    return x.transpose(2, 0, 1).astype(\"float32\")\n",
    "\n",
    "\n",
    "def mask2rle(img):\n",
    "    \"\"\"\n",
    "    Convert mask to rle.\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    \"\"\"\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def post_process(probability, threshold, min_size):\n",
    "    \"\"\"\n",
    "    This is slightly different from other kernels as we draw convex hull here itself.\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    mask = (cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1])\n",
    "    mask = draw_convex_hull(mask.astype(np.uint8))\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros((350, 525), np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = component == c\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(\n",
    "            scale_limit=0.5,\n",
    "            rotate_limit=0,\n",
    "            shift_limit=0.1,\n",
    "            p=0.5,\n",
    "            border_mode=0\n",
    "        ),\n",
    "        albu.GridDistortion(p=0.5),\n",
    "        albu.Resize(320, 640),\n",
    "        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.Resize(320, 640),\n",
    "        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "\n",
    "def dice(img1, img2):\n",
    "    img1 = np.asarray(img1).astype(np.bool)\n",
    "    img2 = np.asarray(img2).astype(np.bool)\n",
    "\n",
    "    intersection = np.logical_and(img1, img2)\n",
    "\n",
    "    return 2.0 * intersection.sum() / (img1.sum() + img2.sum())\n",
    "\n",
    "def dice_no_threshold(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    https://catalyst-team.github.io/catalyst/_modules/catalyst/dl/utils/criterion/dice.html\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    dice = 2 * intersection / (union + eps)\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:36.510018Z",
     "start_time": "2020-01-22T10:53:36.475245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "MODEL_NUM = 0 # in K-fold\n",
    "N_FOLDS = 5 # in K-fold\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 8\n",
    "n_classes = 4\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset => train, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:38.939160Z",
     "start_time": "2020-01-22T10:53:36.512021Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "train_df = pd.read_csv(f'{path}/train.csv')\n",
    "sub_df = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "\n",
    "train_df[\"label\"] = train_df[\"Image_Label\"].apply(lambda x: x.split(\"_\")[1])\n",
    "train_df[\"im_id\"] = train_df[\"Image_Label\"].apply(lambda x: x.split(\"_\")[0])\n",
    "sub_df[\"label\"] = sub_df[\"Image_Label\"].apply(lambda x: x.split(\"_\")[1])\n",
    "sub_df[\"im_id\"] = sub_df[\"Image_Label\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "if DEBUG:\n",
    "    train_df = train_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:38.980603Z",
     "start_time": "2020-01-22T10:53:38.941378Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "id_mask_count = (\n",
    "    train_df.loc[train_df[\"EncodedPixels\"].isnull() == False, \"Image_Label\"]\n",
    "    .apply(lambda x: x.split(\"_\")[0])\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"img_id\", \"Image_Label\": \"count\"})\n",
    ")\n",
    "\n",
    "ids = id_mask_count[\"img_id\"].values\n",
    "\n",
    "li = [\n",
    "    [train_index, test_index]\n",
    "    for train_index, test_index in StratifiedKFold(\n",
    "        n_splits=N_FOLDS, random_state=SEED, shuffle=True\n",
    "    ).split(ids, id_mask_count[\"count\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.020396Z",
     "start_time": "2020-01-22T10:53:38.983000Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids, valid_ids = ids[li[MODEL_NUM][0]], ids[li[MODEL_NUM][1]]\n",
    "test_ids = sub_df[\"Image_Label\"].apply(lambda x: x.split(\"_\")[0]).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.048138Z",
     "start_time": "2020-01-22T10:53:39.024477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set   ['0011165.jpg' '002be4f.jpg' '0031ae9.jpg' '0035239.jpg' '00498ec.jpg'].. with length 10\n",
      "validation set ['003994e.jpg' '006c5a6.jpg' '008233e.jpg'].. with length 3\n",
      "testing set    ['002f507.jpg' '0035ae9.jpg' '0038327.jpg' '004f759.jpg' '005ba08.jpg'].. with length 3698\n"
     ]
    }
   ],
   "source": [
    "print(f\"training set   {train_ids[:5]}.. with length {len(train_ids)}\")\n",
    "print(f\"validation set {valid_ids[:5]}.. with length {len(valid_ids)}\")\n",
    "print(f\"testing set    {test_ids[:5]}.. with length {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.074635Z",
     "start_time": "2020-01-22T10:53:39.050259Z"
    }
   },
   "outputs": [],
   "source": [
    "# define dataset and dataloader\n",
    "num_workers = 2\n",
    "train_dataset = CloudDataset(\n",
    "    df=train_df,\n",
    "    datatype=\"train\",\n",
    "    img_ids=train_ids,\n",
    "    transforms=get_training_augmentation(),\n",
    ")\n",
    "valid_dataset = CloudDataset(\n",
    "    df=train_df,\n",
    "    datatype=\"valid\",\n",
    "    img_ids=valid_ids,\n",
    "    transforms=get_validation_augmentation(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.120045Z",
     "start_time": "2020-01-22T10:53:39.076321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    \"\"\"(conv => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256, False)\n",
    "        self.up2 = up(512, 128, False)\n",
    "        self.up3 = up(256, 64, False)\n",
    "        self.up4 = up(128, 64, False)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.160530Z",
     "start_time": "2020-01-22T10:53:39.121866Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f_score(pr, gt, beta=1, eps=1e-7, threshold=None, activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pr (torch.Tensor): A list of predicted elements\n",
    "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: IoU (Jaccard) score\n",
    "    \"\"\"\n",
    "\n",
    "    if activation is None or activation == \"none\":\n",
    "        activation_fn = lambda x: x\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_fn = torch.nn.Sigmoid()\n",
    "    elif activation == \"softmax2d\":\n",
    "        activation_fn = torch.nn.Softmax2d()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Activation implemented for sigmoid and softmax2d\"\n",
    "        )\n",
    "\n",
    "    pr = activation_fn(pr)\n",
    "\n",
    "    if threshold is not None:\n",
    "        pr = (pr > threshold).float()\n",
    "\n",
    "\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fp = torch.sum(pr) - tp\n",
    "    fn = torch.sum(gt) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + eps) \\\n",
    "            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# def dice_loss(input, target):\n",
    "#     smooth = 1.\n",
    "\n",
    "#     iflat = input.view(-1)\n",
    "#     tflat = target.view(-1)\n",
    "#     intersection = (iflat * tflat).sum()\n",
    "    \n",
    "#     return 1 - ((2. * intersection + smooth) /\n",
    "#               (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    __name__ = 'dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        return 1 - f_score(y_pr, y_gt, beta=1., \n",
    "                           eps=self.eps, threshold=None, \n",
    "                           activation=self.activation)\n",
    "\n",
    "\n",
    "class BCEDiceLoss(DiceLoss):\n",
    "    __name__ = 'bce_dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-7, activation='sigmoid', lambda_dice=1.0, lambda_bce=1.0):\n",
    "        super().__init__(eps, activation)\n",
    "        if activation == None:\n",
    "            self.bce = nn.BCELoss(reduction='mean')\n",
    "        else:\n",
    "            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.lambda_dice=lambda_dice\n",
    "        self.lambda_bce=lambda_bce\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        dice = super().forward(y_pr, y_gt)\n",
    "        bce = self.bce(y_pr, y_gt)\n",
    "        return (self.lambda_dice*dice) + (self.lambda_bce* bce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T08:22:27.011711Z",
     "start_time": "2020-01-22T08:22:26.989535Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## RAdam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.215263Z",
     "start_time": "2020-01-22T10:53:39.162376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "            \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.417183Z",
     "start_time": "2020-01-22T10:53:39.218397Z"
    }
   },
   "outputs": [],
   "source": [
    "model = UNet(n_channels=3, n_classes=n_classes).float()\n",
    "if is_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:53:39.448436Z",
     "start_time": "2020-01-22T10:53:39.418909Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = BCEDiceLoss(eps=1.0, activation=None)\n",
    "optimizer = RAdam(model.parameters(), lr = 0.005)\n",
    "current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, cooldown=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T10:58:35.923592Z",
     "start_time": "2020-01-22T10:56:29.753786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, train_loss=0]\u001b[A\n",
      "  0%|          | 0/5 [00:11<?, ?it/s, train_loss=1.57]\u001b[A\n",
      " 20%|██        | 1/5 [00:11<00:46, 11.74s/it, train_loss=1.57]\u001b[A\n",
      " 20%|██        | 1/5 [00:22<00:46, 11.74s/it, train_loss=1.57]\u001b[A\n",
      " 40%|████      | 2/5 [00:22<00:34, 11.45s/it, train_loss=1.57]\u001b[A\n",
      " 40%|████      | 2/5 [00:34<00:34, 11.45s/it, train_loss=1.55]\u001b[A\n",
      " 60%|██████    | 3/5 [00:34<00:22, 11.49s/it, train_loss=1.55]\u001b[A\n",
      " 60%|██████    | 3/5 [00:45<00:22, 11.49s/it, train_loss=1.55]\u001b[A\n",
      " 80%|████████  | 4/5 [00:45<00:11, 11.33s/it, train_loss=1.55]\u001b[A\n",
      " 80%|████████  | 4/5 [00:57<00:11, 11.33s/it, train_loss=1.54]\u001b[A\n",
      "100%|██████████| 5/5 [00:57<00:00, 11.47s/it, train_loss=1.54]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, dice_score=0, valid_loss=0]\u001b[A\n",
      "  0%|          | 0/2 [00:04<?, ?it/s, ordered_dice={'valid_loss': 1.6270956993103027, 'dice_score': 0.0}]\u001b[A\n",
      " 50%|█████     | 1/2 [00:04<00:04,  4.43s/it, ordered_dice={'valid_loss': 1.6270956993103027, 'dice_score': 0.0}]\u001b[A\n",
      " 50%|█████     | 1/2 [00:06<00:04,  4.43s/it, ordered_dice={'valid_loss': 1.6417121887207031, 'dice_score': 0.0}]\u001b[A\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.37s/it, ordered_dice={'valid_loss': 1.6417121887207031, 'dice_score': 0.0}]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, train_loss=0]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Training Loss: 1.557840  Validation Loss: 1.631968 Dice Score: 0.000000\n",
      "Validation loss decreased (inf --> 1.631968).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:13<?, ?it/s, train_loss=1.53]\u001b[A\n",
      " 20%|██        | 1/5 [00:13<00:53, 13.33s/it, train_loss=1.53]\u001b[A\n",
      " 20%|██        | 1/5 [00:23<00:53, 13.33s/it, train_loss=1.52]\u001b[A\n",
      " 40%|████      | 2/5 [00:23<00:37, 12.41s/it, train_loss=1.52]\u001b[A\n",
      " 40%|████      | 2/5 [00:34<00:37, 12.41s/it, train_loss=1.51]\u001b[A\n",
      " 60%|██████    | 3/5 [00:34<00:23, 11.91s/it, train_loss=1.51]\u001b[A\n",
      " 60%|██████    | 3/5 [00:45<00:23, 11.91s/it, train_loss=1.51]\u001b[A\n",
      " 80%|████████  | 4/5 [00:45<00:11, 11.55s/it, train_loss=1.51]\u001b[A\n",
      " 80%|████████  | 4/5 [00:55<00:11, 11.55s/it, train_loss=1.5] \u001b[A\n",
      "100%|██████████| 5/5 [00:55<00:00, 11.10s/it, train_loss=1.5]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, dice_score=0, valid_loss=0]\u001b[A\n",
      "  0%|          | 0/2 [00:04<?, ?it/s, ordered_dice={'valid_loss': 1.5317394733428955, 'dice_score': 0.0}]\u001b[A\n",
      " 50%|█████     | 1/2 [00:04<00:04,  4.23s/it, ordered_dice={'valid_loss': 1.5317394733428955, 'dice_score': 0.0}]\u001b[A\n",
      " 50%|█████     | 1/2 [00:06<00:04,  4.23s/it, ordered_dice={'valid_loss': 1.5624432563781738, 'dice_score': 0.0}]\u001b[A\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.15s/it, ordered_dice={'valid_loss': 1.5624432563781738, 'dice_score': 0.0}]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  Training Loss: 1.515711  Validation Loss: 1.541974 Dice Score: 0.000000\n",
      "Validation loss decreased (1.631968 --> 1.541974).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "dice_score_list = []\n",
    "lr_rate_list = []\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    dice_score = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    bar = tq(train_loader, postfix={\"train_loss\":0.0})\n",
    "    for data, target in bar:\n",
    "        \n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item()})\n",
    "        \n",
    "    model.eval()\n",
    "    del data, target\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_loader, postfix={\"valid_loss\":0.0, \"dice_score\":0.0})\n",
    "        for data, target in bar:\n",
    "            \n",
    "            if is_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "            dice_coef = dice_no_threshold(output.cpu(), target.cpu()).item()\n",
    "            dice_score += dice_coef * data.size(0)\n",
    "            bar.set_postfix(ordered_dice={\"valid_loss\":loss.item(), \"dice_score\":dice_coef})\n",
    "            \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    dice_score = dice_score / len(valid_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    dice_score_list.append(dice_score)\n",
    "    lr_rate_list.append([param_group['lr'] for param_group in optimizer.param_groups])\n",
    "    \n",
    "    print('Epoch: {}  Training Loss: {:.6f}  Validation Loss: {:.6f} Dice Score: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss, dice_score))\n",
    "    \n",
    "#     if valid_loss <= valid_loss_min:\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "#         valid_loss_min,\n",
    "#         valid_loss))\n",
    "#         torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "#         valid_loss_min = valid_loss\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T11:01:26.293680Z",
     "start_time": "2020-01-22T11:01:26.272010Z"
    }
   },
   "outputs": [],
   "source": [
    "class_params = {}\n",
    "for class_id in range(4):\n",
    "    print(class_id)\n",
    "    attempts = []\n",
    "    for t in range(0, 100, 5):\n",
    "        t /= 100\n",
    "        for ms in [0, 100, 1200, 5000, 10000, 30000]:\n",
    "            masks, d = [], []\n",
    "            for i in range(class_id, len(probabilities), 4):\n",
    "                probability = probabilities[i]\n",
    "                predict, num_predict = post_process(probability, t, ms)\n",
    "                masks.append(predict)\n",
    "            for i, j in zip(masks, valid_masks[class_id::4]):\n",
    "                if (i.sum() == 0) & (j.sum() == 0):\n",
    "                    d.append(1)\n",
    "                else:\n",
    "                    d.append(dice(i, j))\n",
    "            attempts.append((t, ms, np.mean(d)))\n",
    "\n",
    "    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n",
    "    attempts_df = attempts_df.sort_values('dice', ascending=False)\n",
    "    print(attempts_df.head())\n",
    "    best_threshold = attempts_df['threshold'].values[0]\n",
    "    best_size = attempts_df['size'].values[0]\n",
    "    class_params[class_id] = (best_threshold, best_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('py36': conda)",
   "language": "python",
   "name": "python37564bitpy36condaca33cc43c1994fa08616385a83729a3a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
